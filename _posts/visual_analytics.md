---
layout: post
author: jdebrabant
title: "Towards Interactive Enterprise Visual Analytics Platforms"
published: false
date: 2015-05-05
---

In the last few years, visual data analysis has become a key tool in the process of turning raw data into actionable insight. This is not surprising, as visualizations can provide an intuitive interface for business and non-business users to quickly inspect huge amounts of data. In addition, visualizing data is a great way to discover insights when you don’t know what you’re looking for. Unfortunately, these systems have long been designed under the assumption that much, if not all, of the data would fit in memory. As data size growth has continued to outpace the growth of memory capacities, this assumption is no longer valid in many cases. While the increase in data size is true for all areas of data analysis, not just visualization, visualization systems are more vulnerable because of one key reason: visual analysis is inherently “human-in-the-loop”. This means that there is a loop where a visual result is presented, the user analyzes and interacts with it, which either leads to an added insight or a follow-up query. Most algorithmic approaches to data analysis do not follow this same paradigm and can be considered more offline. Because of this, any increase in computation time can lead to a significant decrease in interactivity, which can eventually make a tool unusable. Algorithmic analysis techniques that don’t involve interaction with a user are more durable in this regard. 

To address this issue, many techniques have been proposed for aggregating data in the visualization system. In this architecture, the visualization interface frontend sits on top of a database management system (DBMS) backend and issues queries to the DBMS for the data that is currently needed for a visualization. The raw data is sent from the DBMS to the frontend, where it is reduced (via numerous sampling and aggregation techniques) to a form that can be visualized. The visualization community has viewed the backend simply as a store for raw data, and has focused efforts on different visualization-specific data reduction techniques. This decoupled approach to the frontend and backend is flawed in several ways. For one, the frontend system is duplicating the reduction and aggregation operations that are standard on any DBMS. Any visualization frontend is going to have to filter and aggregate data, and determine the optimal ordering for a given set of operations, which are operations that DBMS was designed to do. Second, if the frontend is doing significant aggregation and post-processing of results from the backend, it becomes impossible to effectively cache these rendered visual components at the backend. This means that caching anything but raw unprocessed data must be done at the frontend, which is typically ill-suited for such heavyweight caching. Ideally, the backend would cache data in the exact form needed by the frontend, which requires the backend to have access to the final processed rendered visual components by doing this processing directly, before data is sent to the frontend. Finally, delivering large amounts of raw data to the frontend puts a lot of pressure on both the backend and network resources and violates one of the key principles of managing big data: avoiding the movement of data. 


### Next Generation Platforms

Recent proposals have called for tighter integration between frontend visualization systems and backend data management systems. In the paper entitled [M4: A Visualization-Oriented Time Series Data Aggregation](http://www.vldb.org/pvldb/vol7/p797-jugel.pdf), the authors present a system for visualizing aggregated time-series data. Their goal is to push the aggregation and filtering computations down to the underlying DBMS, but to do so in a way that is optimized for the type of visualization that is being produced at the frontend, in this case a line chart. They demonstrate that by respecting the process of line rasterization necessary to render a line chart. Their M4 aggregation algorithm can achieve approximate visualization results that were nearly identical to the baseline visualization with orders of magnitude faster query times. Importantly, the relative difference in query runtimes between the baseline and M4 got even better as more rows were visualized. This is important, as it is this increase in query times as data grows that is the primary blocker for interactive visualization of large datasets. 

In a recent vision paper [The Case For Data Visualization Managment Systems](http://www.mit.edu/~eugenewu/files/papers/ermac-vldb14.pdf), the authors propose a more visualization-agnostic approach, and argue for a general purpose database visualization management system (DVMS) that accepts a set of visualization primitives and compiles them to the corresponding relational operators. The goal is to provide an interface to the DBMS that is able to express a broad range of typical query workloads while being able to leverage the performance of the relational execution engine for common data operations like aggregation and filtering. To do this, the authors propose a declarative visualization language that is compiled down to a physical visualization plan that includes a sequence of relational operators in addition to materialized visualization components that have already been rendered. For most data management systems, the user has no input (or feedback) into how long a given query will take. For visualizations, this could be problematic, as interactivity is typically a key feature. The proposed DVMS addresses this by allowing the visualization system to pass budget constraints (e.g., time limits) as parameters for a query. The system could then work within these budget constraints to provide best effort approximate results. However, the tradeoff between the budget constraints and providing meaningful approximate results within those constraints remains unclear. 

### Principles of Interactive Visual Analytics  

One of the key insights that both of the above papers build on is that visualization is a unique form of analytics with its own set of constraints. This requires a new approach to data management for visualization systems and opens the door for a unique set of optimizations. Below are set of visualization principles and constraints that are relevant when aiming to design scalable visual analytics platforms. 

* Low latency is a must.  Because visual exploration of data is a human-in-the-loop process, the queries that feed the visual pipeline cannot be long-running. This contrasts with most analytics requirements, where high latencies are OK and batch processing is the norm. 
* Approximate results are OK. Visual analysis is limited by the number of pixels on the screen, so returning billions of results from the backend is unlikely to be useful. Many data reduction techniques, including aggregation and sampling, have been proposed and should be utilized. 
* Aggressive precomputation of visual components is possible. This is related to the low latency requirement in several ways. For one, in certain situations precomputation of visual components will be necessary for interactivity. Second, the same reason that interactivity is important (i.e., the human-in-the-loop nature of visualization), is also what makes precomputation possible. There are two periods of time when precomputation is possible, before and during a user session. Because users are not always on, there’s likely to be periods of times (e.g., nights, weekends) when the system is idle and precomputation can be done. In addition, when a user interacts with a system, there is inherently the notion of think time or user time, where the user analyzes results and interacts with the current visualization without requesting new data. This time can be utilized to prefetch or precompute results that the user may be interested in next. Aggressive precomputation is the primary techniques used by [imMens](http://vis.stanford.edu/projects/immens/), a system for interactively querying billion row datasets. 
* Data semantics matter. Typically, data management systems view data merely as a set of columns and rows that can be accessed in any number of ways. However, for visual analysis, the semantics of data are important, as they dictate how data will be visualized and explored. This information can be invaluable in determining how best to precompute and approximate results. For example, in the M4 aggregation technique discussed above, it is the semantics of how time series data is visualized (i.e., rasterized line charts) that allowed aggressive resolution reduction over baseline approaches. Similarly, in the paper entitled [Interactive Data Exploration Using Semantic Windows](http://cs.brown.edu/~akalinin/papers/sem-windows.pdf ), the authors show that allowing users to specifiy windows of interest semantically, either by area or by content, the DBMS can leverage this semantic information in aggressive prefetching and caching techniques to significantly reduce overall query latency. 

While we believe the principles above can be applied across a wide range of visualization platforms, there are no doubt many more optimizations possible, in particular when interfaces are designed custom for certain domains. All of these principles point to the need for a tighter coupling of visual interfaces and the data management systems they sit on. Only then can we achieve truly interactive visual analytics platforms. 

At ActionIQ, we’re applying the principles above to build a next generation visual analytics platform for enterprise marketing data. If the intersection of data management and visual analytics excites you, we’d love to [hear from you](mailto:blog@actioniq.co)




